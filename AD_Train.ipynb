{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGUBSgA06fUj",
        "outputId": "7d335ff0-842a-4c72-85bf-b21f59b8850a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"/content/drive/MyDrive/\"\n",
        "import os\n",
        "os.chdir(root_dir + 'AD')"
      ],
      "metadata": {
        "id": "q4JT3XcD6kGR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils import paths\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras import Model, layers\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Flatten, Activation, GlobalAveragePooling2D,Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "# #perform denoising\n",
        "# def denoise(image):\n",
        "\n",
        "#     #denoising using Non-local mean algorithm\n",
        "#     out = cv2.fastNlMeansDenoisingColored(image,None,10,10,7,21)\n",
        "#     return out\n",
        "\n",
        "\n",
        "# data = []\n",
        "# label = []\n",
        "# print(\"[INFO] loading images...\")\n",
        "# img_dir=sorted(list(paths.list_images(\"dataset\")))\n",
        "# random.shuffle(img_dir)\n",
        "# print(\"[INFO]  Preprocessing...\")\n",
        "# print(\"total-->\",len(img_dir))\n",
        "# tot=len(img_dir)\n",
        "# count=0\n",
        "# for i in img_dir:\n",
        "#     img = cv2.imread(i)\n",
        "#     img=cv2.resize(img,(128,128))\n",
        "\n",
        "#     # print(img.shape)\n",
        "#     img=denoise(img)\n",
        "#     # convert it to grayscale\n",
        "#     img_yuv = cv2.cvtColor(img,cv2.COLOR_BGR2YUV)\n",
        "\n",
        "#     # apply histogram equalization \n",
        "#     img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
        "#     hist_eq = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
        "#     # cv2.imshow(\"equalizeHist\", np.hstack((img, hist_eq)))\n",
        "#     # cv2.waitKey(0)\n",
        "\n",
        "#     # print(img.shape)\n",
        "#     # img = skimage.transform.resize(img, (224, 224, 3))\n",
        "#     # hist_eq_img = hist_eq.reshape(1,128,128,3)\n",
        "#     # IMG = np.array(hist_eq)\n",
        "\n",
        "#     #ft1 = dense_feat.predict(IMG) \n",
        "#     # ft2 = res_feat.predict(IMG) \n",
        "#     # res=np.concatenate((ft1, ft2), axis=1)\n",
        " \n",
        "#     data.append(hist_eq) \n",
        "#     lb=i.split(os.path.sep)[-2]\n",
        "#     if lb=='MildDemented':\n",
        "#         label.append(0)\n",
        "#     elif lb=='ModerateDemented':\n",
        "#         label.append(1)\n",
        "#     elif lb=='NonDemented':\n",
        "#         label.append(2)\n",
        "#     elif lb=='VeryMildDemented':\n",
        "#         label.append(3)\n",
        "#     print(count,\"/\",tot)\n",
        "#     count+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# pickle.dump(data,open('feats.pkl','wb'))\n",
        "# pickle.dump(label,open('labels.pkl','wb'))\n",
        "\n",
        "\n",
        "data=pickle.load(open('feats.pkl','rb'))\n",
        "labels=pickle.load(open('labels.pkl','rb'))\n",
        "print(\"****************\")\n",
        "print(len(data))\n",
        "print(len(labels))\n",
        "\n",
        "data=np.array(data)\n",
        "labels=np.array(labels) \n",
        "\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#perform train-test splitting\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, random_state=0, shuffle=True,test_size=0.2)\n",
        "print(\"\\nTraining Set\")\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(\"\\nTesting Set\")\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "#perform normalization\n",
        "x_train=x_train/255\n",
        "x_test=x_test/255\n",
        "\n",
        "get_a=x_train.shape[0]\n",
        "get_b=x_train.shape[1]\n",
        "get_c=x_train.shape[2]\n",
        "get_d=x_train.shape[3]\n",
        "\n",
        "x_train=x_train.reshape(get_a,get_b*get_c*get_d)\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "print('Original dataset shape %s' % Counter(y_train))\n",
        "smt = SMOTETomek(random_state=42)\n",
        "x_train, y_train = smt.fit_resample(x_train, y_train)\n",
        "print('Resampled dataset shape %s' % Counter(y_train))\n",
        "\n",
        "#perform Label encoding (return binary)\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "#initialize\n",
        "label_as_binary = LabelBinarizer()\n",
        "y_train = label_as_binary.fit_transform(y_train)\n",
        "y_test = label_as_binary.fit_transform(y_test)\n",
        "\n",
        "x_train=x_train.reshape(8164,128,128,3)\n",
        "\n",
        "from model import Custom\n",
        "#function call\n",
        "model=Custom()\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "#compiling the mode;\n",
        "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"acc\"])\n",
        "\n",
        "#saving the model\n",
        "checkpoint=ModelCheckpoint(\"Project_Saved_Models/trained_model.h5\",\n",
        "                           monitor=\"acc\",\n",
        "                           save_best_only=True,\n",
        "                           verbose=1)\n",
        "\n",
        "\n",
        "# # Generates batches of image data with data augmentation\n",
        "# datagen = ImageDataGenerator(rotation_range=360, # Degree range for random rotations\n",
        "#                         width_shift_range=0.2, # Range for random horizontal shifts\n",
        "#                         height_shift_range=0.2, # Range for random vertical shifts\n",
        "#                         zoom_range=0.2, # Range for random zoom\n",
        "#                         horizontal_flip=True, # Randomly flip inputs horizontally\n",
        "#                         vertical_flip=True) # Randomly flip inputs vertically\n",
        "\n",
        "# datagen.fit(x_train)\n",
        "# # Fits the model on batches with real-time data augmentation\n",
        "# history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=4),\n",
        "#                steps_per_epoch=x_train.shape[0] // 4,\n",
        "#                epochs=200,\n",
        "#                callbacks=[checkpoint],\n",
        "#                validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "#training\n",
        "history= model.fit(x_train,y_train,epochs=30,batch_size=8,validation_data=(x_test,y_test),callbacks=[checkpoint])\n",
        "\n",
        "#plotting\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig(\"Project_Extra/acc_plot.png\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig(\"Project_Extra/loss_plot.png\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# final_loss, final_accuracy = model.evaluate(x_test, y_test)\n",
        "#### print('Final Loss: {}, Final Accuracy: {}'.format(final_loss, final_accuracy))\n",
        "\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "\n",
        "disease_types=['MildDemented', 'ModerateDemented','NonDemented','VeryMildDemented']\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(12, 12))\n",
        "ax = sns.heatmap(cm, cmap=plt.cm.Greens, annot=True, square=True, xticklabels=disease_types, yticklabels=disease_types)\n",
        "ax.set_ylabel('Actual', fontsize=40)\n",
        "ax.set_xlabel('Predicted', fontsize=40)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3AnPQVzC6lk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}